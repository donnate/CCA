{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CCA_SSG, LogReg\n",
    "from aug2 import gae_aug\n",
    "from dataset import load\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "from GAE import GraphAutoencoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from util import mask_test_edges_dgl\n",
    "import dgl\n",
    "import torch.nn.functional as F\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser.add_argument('--gpu', type=int, default=0, help='GPU index.')\n",
    "# parser.add_argument('--use_mlp', action='store_true', default=False, help='Use MLP instead of GNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_para(adj):\n",
    "    pos_weight = ((adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum())\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "    weight_mask = adj.view(-1) == 1\n",
    "    weight_tensor = th.ones(weight_mask.size(0))\n",
    "    weight_tensor[weight_mask] = pos_weight\n",
    "    return weight_tensor, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch=000, loss=0.6931\n",
      "Epoch=001, loss=0.6924\n",
      "Epoch=002, loss=0.6904\n",
      "Epoch=003, loss=0.6869\n",
      "Epoch=004, loss=0.6828\n",
      "Epoch=005, loss=0.6799\n",
      "Epoch=006, loss=0.6788\n",
      "Epoch=007, loss=0.6765\n",
      "Epoch=008, loss=0.6723\n",
      "Epoch=009, loss=0.6687\n",
      "Epoch=010, loss=0.6657\n",
      "Epoch=011, loss=0.6623\n",
      "Epoch=012, loss=0.6580\n",
      "Epoch=013, loss=0.6528\n",
      "Epoch=014, loss=0.6476\n",
      "Epoch=015, loss=0.6424\n",
      "Epoch=016, loss=0.6367\n",
      "Epoch=017, loss=0.6298\n",
      "Epoch=018, loss=0.6221\n",
      "Epoch=019, loss=0.6142\n",
      "Epoch=020, loss=0.6058\n",
      "Epoch=021, loss=0.5966\n",
      "Epoch=022, loss=0.5870\n",
      "Epoch=023, loss=0.5776\n",
      "Epoch=024, loss=0.5692\n",
      "Epoch=025, loss=0.5619\n",
      "Epoch=026, loss=0.5563\n",
      "Epoch=027, loss=0.5525\n",
      "Epoch=028, loss=0.5501\n",
      "Epoch=029, loss=0.5478\n",
      "Epoch=030, loss=0.5450\n",
      "Epoch=031, loss=0.5409\n",
      "Epoch=032, loss=0.5357\n",
      "Epoch=033, loss=0.5301\n",
      "Epoch=034, loss=0.5248\n",
      "Epoch=035, loss=0.5204\n",
      "Epoch=036, loss=0.5172\n",
      "Epoch=037, loss=0.5150\n",
      "Epoch=038, loss=0.5135\n",
      "Epoch=039, loss=0.5127\n",
      "Epoch=040, loss=0.5126\n",
      "Epoch=041, loss=0.5127\n",
      "Epoch=042, loss=0.5126\n",
      "Epoch=043, loss=0.5124\n",
      "Epoch=044, loss=0.5119\n",
      "Epoch=045, loss=0.5107\n",
      "Epoch=046, loss=0.5092\n",
      "Epoch=047, loss=0.5075\n",
      "Epoch=048, loss=0.5058\n",
      "Epoch=049, loss=0.5043\n",
      "Epoch=050, loss=0.5030\n",
      "Epoch=051, loss=0.5019\n",
      "Epoch=052, loss=0.5012\n",
      "Epoch=053, loss=0.5007\n",
      "Epoch=054, loss=0.5003\n",
      "Epoch=055, loss=0.4999\n",
      "Epoch=056, loss=0.4996\n",
      "Epoch=057, loss=0.4992\n",
      "Epoch=058, loss=0.4987\n",
      "Epoch=059, loss=0.4982\n",
      "Epoch=060, loss=0.4977\n",
      "Epoch=061, loss=0.4971\n",
      "Epoch=062, loss=0.4965\n",
      "Epoch=063, loss=0.4960\n",
      "Epoch=064, loss=0.4955\n",
      "Epoch=065, loss=0.4950\n",
      "Epoch=066, loss=0.4946\n",
      "Epoch=067, loss=0.4943\n",
      "Epoch=068, loss=0.4940\n",
      "Epoch=069, loss=0.4937\n",
      "Epoch=070, loss=0.4934\n",
      "Epoch=071, loss=0.4931\n",
      "Epoch=072, loss=0.4928\n",
      "Epoch=073, loss=0.4925\n",
      "Epoch=074, loss=0.4922\n",
      "Epoch=075, loss=0.4918\n",
      "Epoch=076, loss=0.4915\n",
      "Epoch=077, loss=0.4912\n",
      "Epoch=078, loss=0.4909\n",
      "Epoch=079, loss=0.4906\n",
      "Epoch=080, loss=0.4904\n",
      "Epoch=081, loss=0.4901\n",
      "Epoch=082, loss=0.4899\n",
      "Epoch=083, loss=0.4896\n",
      "Epoch=084, loss=0.4894\n",
      "Epoch=085, loss=0.4891\n",
      "Epoch=086, loss=0.4888\n",
      "Epoch=087, loss=0.4885\n",
      "Epoch=088, loss=0.4882\n",
      "Epoch=089, loss=0.4879\n",
      "Epoch=090, loss=0.4876\n",
      "Epoch=091, loss=0.4874\n",
      "Epoch=092, loss=0.4871\n",
      "Epoch=093, loss=0.4868\n",
      "Epoch=094, loss=0.4864\n",
      "Epoch=095, loss=0.4861\n",
      "Epoch=096, loss=0.4858\n",
      "Epoch=097, loss=0.4854\n",
      "Epoch=098, loss=0.4850\n",
      "Epoch=099, loss=0.4846\n",
      "Epoch=100, loss=0.4841\n",
      "Epoch=101, loss=0.4836\n",
      "Epoch=102, loss=0.4831\n",
      "Epoch=103, loss=0.4826\n",
      "Epoch=104, loss=0.4819\n",
      "Epoch=105, loss=0.4813\n",
      "Epoch=106, loss=0.4805\n",
      "Epoch=107, loss=0.4797\n",
      "Epoch=108, loss=0.4789\n",
      "Epoch=109, loss=0.4779\n",
      "Epoch=110, loss=0.4768\n",
      "Epoch=111, loss=0.4757\n",
      "Epoch=112, loss=0.4744\n",
      "Epoch=113, loss=0.4731\n",
      "Epoch=114, loss=0.4717\n",
      "Epoch=115, loss=0.4702\n",
      "Epoch=116, loss=0.4687\n",
      "Epoch=117, loss=0.4672\n",
      "Epoch=118, loss=0.4658\n",
      "Epoch=119, loss=0.4644\n",
      "Epoch=120, loss=0.4632\n",
      "Epoch=121, loss=0.4621\n",
      "Epoch=122, loss=0.4612\n",
      "Epoch=123, loss=0.4604\n",
      "Epoch=124, loss=0.4597\n",
      "Epoch=125, loss=0.4591\n",
      "Epoch=126, loss=0.4585\n",
      "Epoch=127, loss=0.4578\n",
      "Epoch=128, loss=0.4572\n",
      "Epoch=129, loss=0.4567\n",
      "Epoch=130, loss=0.4562\n",
      "Epoch=131, loss=0.4557\n",
      "Epoch=132, loss=0.4553\n",
      "Epoch=133, loss=0.4550\n",
      "Epoch=134, loss=0.4546\n",
      "Epoch=135, loss=0.4543\n",
      "Epoch=136, loss=0.4539\n",
      "Epoch=137, loss=0.4535\n",
      "Epoch=138, loss=0.4530\n",
      "Epoch=139, loss=0.4526\n",
      "Epoch=140, loss=0.4522\n",
      "Epoch=141, loss=0.4519\n",
      "Epoch=142, loss=0.4516\n",
      "Epoch=143, loss=0.4514\n",
      "Epoch=144, loss=0.4512\n",
      "Epoch=145, loss=0.4510\n",
      "Epoch=146, loss=0.4508\n",
      "Epoch=147, loss=0.4506\n",
      "Epoch=148, loss=0.4504\n",
      "Epoch=149, loss=0.4502\n",
      "Epoch=150, loss=0.4500\n",
      "Epoch=151, loss=0.4498\n",
      "Epoch=152, loss=0.4496\n",
      "Epoch=153, loss=0.4494\n",
      "Epoch=154, loss=0.4493\n",
      "Epoch=155, loss=0.4491\n",
      "Epoch=156, loss=0.4490\n",
      "Epoch=157, loss=0.4488\n",
      "Epoch=158, loss=0.4487\n",
      "Epoch=159, loss=0.4486\n",
      "Epoch=160, loss=0.4484\n",
      "Epoch=161, loss=0.4483\n",
      "Epoch=162, loss=0.4481\n",
      "Epoch=163, loss=0.4480\n",
      "Epoch=164, loss=0.4478\n",
      "Epoch=165, loss=0.4477\n",
      "Epoch=166, loss=0.4475\n",
      "Epoch=167, loss=0.4474\n",
      "Epoch=168, loss=0.4473\n",
      "Epoch=169, loss=0.4471\n",
      "Epoch=170, loss=0.4470\n",
      "Epoch=171, loss=0.4469\n",
      "Epoch=172, loss=0.4467\n",
      "Epoch=173, loss=0.4466\n",
      "Epoch=174, loss=0.4464\n",
      "Epoch=175, loss=0.4463\n",
      "Epoch=176, loss=0.4461\n",
      "Epoch=177, loss=0.4460\n",
      "Epoch=178, loss=0.4458\n",
      "Epoch=179, loss=0.4457\n",
      "Epoch=180, loss=0.4455\n",
      "Epoch=181, loss=0.4453\n",
      "Epoch=182, loss=0.4451\n",
      "Epoch=183, loss=0.4450\n",
      "Epoch=184, loss=0.4448\n",
      "Epoch=185, loss=0.4445\n",
      "Epoch=186, loss=0.4443\n",
      "Epoch=187, loss=0.4441\n",
      "Epoch=188, loss=0.4438\n",
      "Epoch=189, loss=0.4435\n",
      "Epoch=190, loss=0.4433\n",
      "Epoch=191, loss=0.4430\n",
      "Epoch=192, loss=0.4426\n",
      "Epoch=193, loss=0.4423\n",
      "Epoch=194, loss=0.4419\n",
      "Epoch=195, loss=0.4415\n",
      "Epoch=196, loss=0.4411\n",
      "Epoch=197, loss=0.4407\n",
      "Epoch=198, loss=0.4403\n",
      "Epoch=199, loss=0.4398\n"
     ]
    }
   ],
   "source": [
    "graph, feat, labels, num_class, train_idx, val_idx, test_idx = load('cora')\n",
    "adj = graph.adj().to_dense()\n",
    "\n",
    "weight_tensor, norm = compute_loss_para(adj)\n",
    "    \n",
    "in_dim = feat.shape[1]\n",
    "z_dim = 16 \n",
    "h_dim = [32]\n",
    "\n",
    "sampler = GraphAutoencoder([in_dim, z_dim, h_dim])\n",
    "\n",
    "loss_fn = F.binary_cross_entropy\n",
    "optimizer = th.optim.Adam(sampler.parameters(), lr=1e-2) # , weight_decay=5e-4\n",
    "\n",
    "for epoch in range(200):\n",
    "    sampler.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reconstruction = sampler(graph, feat)\n",
    "\n",
    "    loss = norm*loss_fn(reconstruction.view(-1), adj.view(-1), weight = weight_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch={:03d}, loss={:.4f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9163, 0.5949, 0.8117,  ..., 0.6599, 0.8985, 0.8761],\n",
       "        [0.5949, 0.8745, 0.8659,  ..., 0.4400, 0.6388, 0.5919],\n",
       "        [0.8117, 0.8659, 0.9165,  ..., 0.4713, 0.8278, 0.7814],\n",
       "        ...,\n",
       "        [0.6599, 0.4400, 0.4713,  ..., 0.7444, 0.5994, 0.5999],\n",
       "        [0.8985, 0.6388, 0.8278,  ..., 0.5994, 0.8950, 0.8712],\n",
       "        [0.8761, 0.5919, 0.7814,  ..., 0.5999, 0.8712, 0.8485]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction\n",
    "rec = reconstruction.detach()\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(edges_pos, edges_neg, adj_rec):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    adj_rec = adj_rec.cpu()\n",
    "    # Predict on test set of edges\n",
    "    preds = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]].item()))\n",
    "\n",
    "    preds_neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]].data))\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "graph, feat, labels, num_class, train_idx, val_idx, test_idx = load('cora')\n",
    "adj_orig = graph.adj().to_dense()\n",
    "train_edge_idx, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges_dgl(graph, adj_orig)\n",
    "\n",
    "# create train graph\n",
    "train_edge_idx = th.tensor(train_edge_idx)\n",
    "train_graph = dgl.edge_subgraph(graph, train_edge_idx, preserve_nodes=True)\n",
    "\n",
    "# add self loop\n",
    "#train_graph = dgl.remove_self_loop(train_graph)\n",
    "#train_graph = dgl.add_self_loop(train_graph)\n",
    "#n_edges = train_graph.number_of_edges()\n",
    "#adj = train_graph.adjacency_matrix().to_dense()\n",
    "\n",
    "# normalization\n",
    "#degs = train_graph.in_degrees().float()\n",
    "#norm = th.pow(degs, -0.5)\n",
    "#norm[th.isinf(norm)] = 0\n",
    "#train_graph.ndata['norm'] = norm.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=000, loss=-307.7884\n",
      "Epoch=001, loss=-357.1543\n",
      "Epoch=002, loss=-363.2691\n",
      "Epoch=003, loss=-364.5531\n",
      "Epoch=004, loss=-377.0336\n",
      "Epoch=005, loss=-388.0518\n",
      "Epoch=006, loss=-398.3110\n",
      "Epoch=007, loss=-391.5375\n",
      "Epoch=008, loss=-411.2771\n",
      "Epoch=009, loss=-422.5592\n",
      "Epoch=010, loss=-409.8762\n",
      "Epoch=011, loss=-415.7951\n",
      "Epoch=012, loss=-410.1704\n",
      "Epoch=013, loss=-420.2153\n",
      "Epoch=014, loss=-419.0777\n",
      "Epoch=015, loss=-429.3650\n",
      "Epoch=016, loss=-420.8496\n",
      "Epoch=017, loss=-418.1948\n",
      "Epoch=018, loss=-424.1335\n",
      "Epoch=019, loss=-428.0111\n",
      "Epoch=020, loss=-429.7409\n",
      "Epoch=021, loss=-436.2377\n",
      "Epoch=022, loss=-430.6186\n",
      "Epoch=023, loss=-429.4095\n",
      "Epoch=024, loss=-435.1780\n",
      "Epoch=025, loss=-441.9705\n",
      "Epoch=026, loss=-437.6365\n",
      "Epoch=027, loss=-434.6889\n",
      "Epoch=028, loss=-442.6764\n",
      "Epoch=029, loss=-439.3217\n",
      "Epoch=030, loss=-438.4502\n",
      "Epoch=031, loss=-448.4251\n",
      "Epoch=032, loss=-439.7124\n",
      "Epoch=033, loss=-445.2492\n",
      "Epoch=034, loss=-446.3977\n",
      "Epoch=035, loss=-448.8382\n",
      "Epoch=036, loss=-449.9593\n",
      "Epoch=037, loss=-449.0884\n",
      "Epoch=038, loss=-446.6965\n",
      "Epoch=039, loss=-451.8048\n",
      "Epoch=040, loss=-452.7666\n",
      "Epoch=041, loss=-453.8521\n",
      "Epoch=042, loss=-445.8786\n",
      "Epoch=043, loss=-456.8782\n",
      "Epoch=044, loss=-450.6715\n",
      "Epoch=045, loss=-451.3873\n",
      "Epoch=046, loss=-450.4564\n",
      "Epoch=047, loss=-456.1488\n",
      "Epoch=048, loss=-450.6451\n",
      "Epoch=049, loss=-455.9069\n",
      "Epoch=050, loss=-452.0118\n",
      "Epoch=051, loss=-458.3206\n",
      "Epoch=052, loss=-453.3043\n",
      "Epoch=053, loss=-449.0674\n",
      "Epoch=054, loss=-458.4509\n",
      "Epoch=055, loss=-454.9138\n",
      "Epoch=056, loss=-458.8147\n",
      "Epoch=057, loss=-453.4873\n",
      "Epoch=058, loss=-459.8939\n",
      "Epoch=059, loss=-455.5039\n"
     ]
    }
   ],
   "source": [
    "in_dim = feat.shape[1]\n",
    "\n",
    "hid_dim = 512\n",
    "out_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = CCA_SSG(in_dim, hid_dim, out_dim, n_layers, use_mlp=False)\n",
    "lr1 = 1e-3 \n",
    "wd1 = 0\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=lr1, weight_decay=wd1)\n",
    "\n",
    "N = graph.number_of_nodes()\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    dfr = 0.2\n",
    "    der = 0.2\n",
    "\n",
    "    #graph1, feat1 = random_aug(graph, feat, 0.2, 0.2)\n",
    "    #graph2, feat2 = random_aug(graph, feat, 0.2, 0.2)\n",
    "    \n",
    "    graph1, graph2, feat1, feat2 = gae_aug(rec, graph, feat, 0.20, 0.20)\n",
    "    graph1 = graph1.remove_self_loop().add_self_loop()\n",
    "    graph2 = graph2.remove_self_loop().add_self_loop()\n",
    "\n",
    "    graph1 = graph1.add_self_loop()\n",
    "    graph2 = graph2.add_self_loop()\n",
    "\n",
    "    z1, z2 = model(graph1, feat1, graph2, feat2)\n",
    "\n",
    "    c = th.mm(z1.T, z2)\n",
    "    c1 = th.mm(z1.T, z1)\n",
    "    c2 = th.mm(z2.T, z2)\n",
    "\n",
    "    c = c / N\n",
    "    c1 = c1 / N\n",
    "    c2 = c2 / N\n",
    "\n",
    "    loss_inv = -th.diagonal(c).sum()\n",
    "    iden = th.tensor(np.eye(c.shape[0]))\n",
    "    loss_dec1 = (iden - c1).pow(2).sum()\n",
    "    loss_dec2 = (iden - c2).pow(2).sum()\n",
    "    \n",
    "    lambd = 1e-3\n",
    "    \n",
    "    loss = loss_inv + lambd * (loss_dec1 + loss_dec2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch={:03d}, loss={:.4f}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation ===\n",
      "0.9554196895846904 0.9548354451996761\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Evaluation ===\")\n",
    "graph = train_graph.remove_self_loop().add_self_loop()\n",
    "adj = graph.adj().to_dense()\n",
    "\n",
    "weight_tensor, norm = compute_loss_para(adj)\n",
    "\n",
    "embeds = model.get_embedding(graph, feat)\n",
    "\n",
    "loss_fn = F.binary_cross_entropy\n",
    "output_activation = nn.Sigmoid()\n",
    "logreg = LogReg(embeds.shape[1], adj.shape[1])\n",
    "\n",
    "logits_temp = logreg(embeds)\n",
    "logits = output_activation(th.mm(logits_temp, logits_temp.t()))\n",
    "\n",
    "val_roc, val_ap = get_scores(val_edges, val_edges_false, logits)\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, logits)\n",
    "print(test_roc, test_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation ===\n",
      "Epoch:0, val_ap:0.9559, val_roc:0.956155, test_ap:0.952427, test_roc:0.953064\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:1, val_ap:0.7976, val_roc:0.768706, test_ap:0.781770, test_roc:0.765575\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:2, val_ap:0.7846, val_roc:0.751272, test_ap:0.771366, test_roc:0.751892\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:3, val_ap:0.7962, val_roc:0.765318, test_ap:0.781534, test_roc:0.763747\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:4, val_ap:0.8258, val_roc:0.800593, test_ap:0.807046, test_roc:0.794086\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:5, val_ap:0.8840, val_roc:0.868310, test_ap:0.863820, test_roc:0.859137\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:6, val_ap:0.9365, val_roc:0.930637, test_ap:0.918339, test_roc:0.916968\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:7, val_ap:0.9451, val_roc:0.943737, test_ap:0.930245, test_roc:0.931525\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:8, val_ap:0.9482, val_roc:0.948644, test_ap:0.935673, test_roc:0.939399\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:9, val_ap:0.9511, val_roc:0.953375, test_ap:0.939910, test_roc:0.946047\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:10, val_ap:0.9472, val_roc:0.951049, test_ap:0.936759, test_roc:0.944917\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:11, val_ap:0.9344, val_roc:0.937302, test_ap:0.925967, test_roc:0.934242\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:12, val_ap:0.9248, val_roc:0.925344, test_ap:0.918297, test_roc:0.926049\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:13, val_ap:0.9261, val_roc:0.926288, test_ap:0.917948, test_roc:0.925828\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:14, val_ap:0.9327, val_roc:0.933954, test_ap:0.921806, test_roc:0.930198\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:15, val_ap:0.9398, val_roc:0.941720, test_ap:0.926370, test_roc:0.935194\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:16, val_ap:0.9452, val_roc:0.947341, test_ap:0.930259, test_roc:0.939119\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:17, val_ap:0.9485, val_roc:0.950390, test_ap:0.933278, test_roc:0.941788\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:18, val_ap:0.9506, val_roc:0.951971, test_ap:0.935713, test_roc:0.943663\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:19, val_ap:0.9517, val_roc:0.952853, test_ap:0.937701, test_roc:0.945060\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:20, val_ap:0.9528, val_roc:0.953761, test_ap:0.939294, test_roc:0.946129\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:21, val_ap:0.9538, val_roc:0.954492, test_ap:0.940614, test_roc:0.946932\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:22, val_ap:0.9548, val_roc:0.955374, test_ap:0.941845, test_roc:0.947776\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:23, val_ap:0.9558, val_roc:0.956227, test_ap:0.943068, test_roc:0.948690\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:24, val_ap:0.9567, val_roc:0.956969, test_ap:0.944312, test_roc:0.949700\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:25, val_ap:0.9574, val_roc:0.957570, test_ap:0.945780, test_roc:0.950815\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:26, val_ap:0.9580, val_roc:0.958024, test_ap:0.947257, test_roc:0.952112\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9531\n",
      "Epoch:27, val_ap:0.9583, val_roc:0.958290, test_ap:0.948576, test_roc:0.953261\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9533\n",
      "Epoch:28, val_ap:0.9584, val_roc:0.958290, test_ap:0.949629, test_roc:0.954062\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9541\n",
      "Epoch:29, val_ap:0.9584, val_roc:0.958157, test_ap:0.950192, test_roc:0.954439\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9541\n",
      "Epoch:30, val_ap:0.9584, val_roc:0.958207, test_ap:0.950693, test_roc:0.954672\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9541\n",
      "Epoch:31, val_ap:0.9586, val_roc:0.958305, test_ap:0.951232, test_roc:0.954881\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9549\n",
      "Epoch:32, val_ap:0.9590, val_roc:0.958560, test_ap:0.951770, test_roc:0.955237\n",
      "Linear evaluation AP:0.9524\n",
      "Linear evaluation ROC:0.9552\n",
      "Epoch:33, val_ap:0.9595, val_roc:0.958938, test_ap:0.952465, test_roc:0.955792\n",
      "Linear evaluation AP:0.9525\n",
      "Linear evaluation ROC:0.9558\n",
      "Epoch:34, val_ap:0.9600, val_roc:0.959428, test_ap:0.952954, test_roc:0.956303\n",
      "Linear evaluation AP:0.9530\n",
      "Linear evaluation ROC:0.9563\n",
      "Epoch:35, val_ap:0.9603, val_roc:0.959741, test_ap:0.953515, test_roc:0.956891\n",
      "Linear evaluation AP:0.9535\n",
      "Linear evaluation ROC:0.9569\n",
      "Epoch:36, val_ap:0.9606, val_roc:0.959860, test_ap:0.953904, test_roc:0.957329\n",
      "Linear evaluation AP:0.9539\n",
      "Linear evaluation ROC:0.9573\n",
      "Epoch:37, val_ap:0.9608, val_roc:0.960069, test_ap:0.954126, test_roc:0.957600\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:38, val_ap:0.9611, val_roc:0.960217, test_ap:0.954113, test_roc:0.957624\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:39, val_ap:0.9611, val_roc:0.960152, test_ap:0.953916, test_roc:0.957407\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:40, val_ap:0.9611, val_roc:0.960224, test_ap:0.953483, test_roc:0.957035\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:41, val_ap:0.9610, val_roc:0.960173, test_ap:0.953090, test_roc:0.956648\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:42, val_ap:0.9609, val_roc:0.960148, test_ap:0.952702, test_roc:0.956326\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:43, val_ap:0.9608, val_roc:0.960112, test_ap:0.952377, test_roc:0.956088\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:44, val_ap:0.9608, val_roc:0.960191, test_ap:0.952259, test_roc:0.956074\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:45, val_ap:0.9608, val_roc:0.960353, test_ap:0.952314, test_roc:0.956200\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:46, val_ap:0.9608, val_roc:0.960490, test_ap:0.952423, test_roc:0.956321\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:47, val_ap:0.9608, val_roc:0.960530, test_ap:0.952585, test_roc:0.956465\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:48, val_ap:0.9608, val_roc:0.960555, test_ap:0.952712, test_roc:0.956571\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:49, val_ap:0.9608, val_roc:0.960569, test_ap:0.952830, test_roc:0.956635\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:50, val_ap:0.9608, val_roc:0.960526, test_ap:0.952909, test_roc:0.956680\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:51, val_ap:0.9608, val_roc:0.960469, test_ap:0.953021, test_roc:0.956757\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:52, val_ap:0.9607, val_roc:0.960451, test_ap:0.953255, test_roc:0.956934\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:53, val_ap:0.9607, val_roc:0.960458, test_ap:0.953460, test_roc:0.957104\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:54, val_ap:0.9608, val_roc:0.960490, test_ap:0.953683, test_roc:0.957279\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:55, val_ap:0.9608, val_roc:0.960548, test_ap:0.953939, test_roc:0.957491\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:56, val_ap:0.9608, val_roc:0.960501, test_ap:0.954158, test_roc:0.957657\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:57, val_ap:0.9608, val_roc:0.960512, test_ap:0.954245, test_roc:0.957731\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:58, val_ap:0.9608, val_roc:0.960400, test_ap:0.954291, test_roc:0.957765\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:59, val_ap:0.9608, val_roc:0.960343, test_ap:0.954273, test_roc:0.957719\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:60, val_ap:0.9608, val_roc:0.960217, test_ap:0.954244, test_roc:0.957671\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:61, val_ap:0.9609, val_roc:0.960206, test_ap:0.954199, test_roc:0.957630\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:62, val_ap:0.9608, val_roc:0.960141, test_ap:0.954131, test_roc:0.957597\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:63, val_ap:0.9609, val_roc:0.960256, test_ap:0.954109, test_roc:0.957614\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:64, val_ap:0.9609, val_roc:0.960292, test_ap:0.954038, test_roc:0.957579\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:65, val_ap:0.9610, val_roc:0.960332, test_ap:0.954015, test_roc:0.957572\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:66, val_ap:0.9610, val_roc:0.960379, test_ap:0.953989, test_roc:0.957562\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:67, val_ap:0.9610, val_roc:0.960404, test_ap:0.953907, test_roc:0.957518\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:68, val_ap:0.9610, val_roc:0.960418, test_ap:0.953858, test_roc:0.957496\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:69, val_ap:0.9610, val_roc:0.960465, test_ap:0.953833, test_roc:0.957487\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:70, val_ap:0.9610, val_roc:0.960505, test_ap:0.953879, test_roc:0.957536\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:71, val_ap:0.9611, val_roc:0.960577, test_ap:0.953963, test_roc:0.957631\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9576\n",
      "Epoch:72, val_ap:0.9611, val_roc:0.960620, test_ap:0.954064, test_roc:0.957727\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9577\n",
      "Epoch:73, val_ap:0.9611, val_roc:0.960699, test_ap:0.954140, test_roc:0.957810\n",
      "Linear evaluation AP:0.9541\n",
      "Linear evaluation ROC:0.9578\n",
      "Epoch:74, val_ap:0.9612, val_roc:0.960742, test_ap:0.954245, test_roc:0.957899\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:75, val_ap:0.9611, val_roc:0.960663, test_ap:0.954304, test_roc:0.957938\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:76, val_ap:0.9610, val_roc:0.960638, test_ap:0.954336, test_roc:0.957961\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:77, val_ap:0.9610, val_roc:0.960605, test_ap:0.954395, test_roc:0.958011\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:78, val_ap:0.9609, val_roc:0.960580, test_ap:0.954436, test_roc:0.958039\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:79, val_ap:0.9610, val_roc:0.960591, test_ap:0.954512, test_roc:0.958084\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:80, val_ap:0.9610, val_roc:0.960616, test_ap:0.954578, test_roc:0.958131\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:81, val_ap:0.9610, val_roc:0.960580, test_ap:0.954593, test_roc:0.958146\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:82, val_ap:0.9610, val_roc:0.960577, test_ap:0.954554, test_roc:0.958120\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:83, val_ap:0.9609, val_roc:0.960515, test_ap:0.954547, test_roc:0.958117\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:84, val_ap:0.9610, val_roc:0.960530, test_ap:0.954494, test_roc:0.958070\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:85, val_ap:0.9610, val_roc:0.960505, test_ap:0.954454, test_roc:0.958047\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:86, val_ap:0.9610, val_roc:0.960537, test_ap:0.954444, test_roc:0.958037\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:87, val_ap:0.9610, val_roc:0.960533, test_ap:0.954457, test_roc:0.958052\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:88, val_ap:0.9610, val_roc:0.960544, test_ap:0.954455, test_roc:0.958055\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:89, val_ap:0.9610, val_roc:0.960544, test_ap:0.954460, test_roc:0.958077\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:90, val_ap:0.9610, val_roc:0.960541, test_ap:0.954478, test_roc:0.958093\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:91, val_ap:0.9610, val_roc:0.960551, test_ap:0.954478, test_roc:0.958105\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:92, val_ap:0.9610, val_roc:0.960548, test_ap:0.954508, test_roc:0.958134\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:93, val_ap:0.9610, val_roc:0.960569, test_ap:0.954526, test_roc:0.958151\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:94, val_ap:0.9610, val_roc:0.960587, test_ap:0.954555, test_roc:0.958174\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:95, val_ap:0.9610, val_roc:0.960605, test_ap:0.954602, test_roc:0.958215\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:96, val_ap:0.9610, val_roc:0.960609, test_ap:0.954641, test_roc:0.958244\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:97, val_ap:0.9611, val_roc:0.960652, test_ap:0.954671, test_roc:0.958268\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:98, val_ap:0.9611, val_roc:0.960652, test_ap:0.954684, test_roc:0.958286\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:99, val_ap:0.9611, val_roc:0.960674, test_ap:0.954680, test_roc:0.958280\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:100, val_ap:0.9611, val_roc:0.960638, test_ap:0.954687, test_roc:0.958277\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:101, val_ap:0.9611, val_roc:0.960649, test_ap:0.954684, test_roc:0.958276\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:102, val_ap:0.9611, val_roc:0.960659, test_ap:0.954701, test_roc:0.958290\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:103, val_ap:0.9611, val_roc:0.960674, test_ap:0.954706, test_roc:0.958300\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:104, val_ap:0.9611, val_roc:0.960677, test_ap:0.954696, test_roc:0.958291\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:105, val_ap:0.9611, val_roc:0.960652, test_ap:0.954685, test_roc:0.958282\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:106, val_ap:0.9611, val_roc:0.960656, test_ap:0.954680, test_roc:0.958277\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:107, val_ap:0.9611, val_roc:0.960663, test_ap:0.954672, test_roc:0.958264\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:108, val_ap:0.9611, val_roc:0.960652, test_ap:0.954680, test_roc:0.958272\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:109, val_ap:0.9611, val_roc:0.960670, test_ap:0.954692, test_roc:0.958284\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:110, val_ap:0.9611, val_roc:0.960685, test_ap:0.954704, test_roc:0.958302\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:111, val_ap:0.9611, val_roc:0.960695, test_ap:0.954712, test_roc:0.958315\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:112, val_ap:0.9611, val_roc:0.960688, test_ap:0.954737, test_roc:0.958340\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:113, val_ap:0.9611, val_roc:0.960674, test_ap:0.954762, test_roc:0.958359\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:114, val_ap:0.9611, val_roc:0.960677, test_ap:0.954769, test_roc:0.958367\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:115, val_ap:0.9611, val_roc:0.960688, test_ap:0.954769, test_roc:0.958370\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:116, val_ap:0.9611, val_roc:0.960692, test_ap:0.954780, test_roc:0.958382\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:117, val_ap:0.9611, val_roc:0.960724, test_ap:0.954787, test_roc:0.958391\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:118, val_ap:0.9611, val_roc:0.960728, test_ap:0.954795, test_roc:0.958403\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9579\n",
      "Epoch:119, val_ap:0.9611, val_roc:0.960757, test_ap:0.954795, test_roc:0.958406\n",
      "Linear evaluation AP:0.9542\n",
      "Linear evaluation ROC:0.9584\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-1899713b04ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"=== Evaluation ===\")\n",
    "graph = train_graph.remove_self_loop().add_self_loop()\n",
    "adj = graph.adj().to_dense()\n",
    "\n",
    "weight_tensor, norm = compute_loss_para(adj)\n",
    "\n",
    "embeds = model.get_embedding(graph, feat)\n",
    "\n",
    "''' Linear Evaluation '''\n",
    "logreg = LogReg(embeds.shape[1], adj.shape[1])\n",
    "lr2 = 1e-2\n",
    "wd2 = 1e-4\n",
    "opt = th.optim.Adam(logreg.parameters(), lr=lr2, weight_decay=wd2)\n",
    "\n",
    "loss_fn = F.binary_cross_entropy\n",
    "output_activation = nn.Sigmoid()\n",
    "\n",
    "best_val_roc = 0\n",
    "eval_roc = 0\n",
    "best_val_ap = 0\n",
    "eval_ap = 0\n",
    "    \n",
    "for epoch in range(2000):\n",
    "    logreg.train()\n",
    "    opt.zero_grad()\n",
    "    logits_temp = logreg(embeds)\n",
    "    logits = output_activation(th.mm(logits_temp, logits_temp.t()))\n",
    "    \n",
    "    # pdb.set_trace()\n",
    "    loss = norm*loss_fn(logits.view(-1), adj.view(-1), weight = weight_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    logreg.eval()\n",
    "    with th.no_grad():\n",
    "        val_roc, val_ap = get_scores(val_edges, val_edges_false, logits)\n",
    "        test_roc, test_ap = get_scores(test_edges, test_edges_false, logits)\n",
    "\n",
    "        if val_roc >= best_val_roc:\n",
    "            best_val_roc = val_roc\n",
    "            if test_roc > eval_roc:\n",
    "                eval_roc = test_roc\n",
    "        \n",
    "        if val_ap >= best_val_ap:\n",
    "            best_val_ap = val_ap\n",
    "            if test_ap > eval_ap:\n",
    "                eval_ap = test_ap\n",
    "\n",
    "    print('Epoch:{}, val_ap:{:.4f}, val_roc:{:4f}, test_ap:{:4f}, test_roc:{:4f}'.format(epoch, val_ap, val_roc, test_ap, test_roc))\n",
    "    print('Linear evaluation AP:{:.4f}'.format(eval_ap))\n",
    "    print('Linear evaluation ROC:{:.4f}'.format(eval_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
