{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgraphs-stats-and-ml\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ilgeehong/Desktop/UChicago/Research/Graph/VAE Project/Code/Project Code/TOCCA/wandb/run-20220422_103556-1ehew5gq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/graphs-stats-and-ml/VAE/runs/1ehew5gq\" target=\"_blank\">polar-darkness-3</a></strong> to <a href=\"https://wandb.ai/graphs-stats-and-ml/VAE\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/graphs-stats-and-ml/VAE/runs/1ehew5gq?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc5c1fdcc40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# wandb.init(project=\"VAE\", entity=\"graphs-stats-and-ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from model2 import TOCCA1\n",
    "from model2 import TOCCA\n",
    "from model2 import TOCCA3\n",
    "from model2 import TOCCA4\n",
    "from model2 import TOCCA5\n",
    "from aug import random_aug\n",
    "from dataset import load\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser.add_argument('--gpu', type=int, default=0, help='GPU index.')\n",
    "# parser.add_argument('--use_mlp', action='store_true', default=False, help='Use MLP instead of GNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 19717\n",
      "  NumEdges: 88651\n",
      "  NumFeats: 500\n",
      "  NumClasses: 3\n",
      "  NumTrainingSamples: 60\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch:0, loss=-360.9024, train_acc:0.1667, val_acc:0.180000, test_acc:0.174000\n",
      "test accuracy:0.1740\n",
      "Epoch:1, loss=-378.2430, train_acc:0.1167, val_acc:0.148000, test_acc:0.169000\n",
      "test accuracy:0.1740\n",
      "Epoch:2, loss=-384.7776, train_acc:0.1333, val_acc:0.150000, test_acc:0.165000\n",
      "test accuracy:0.1740\n",
      "Epoch:3, loss=-392.8266, train_acc:0.1167, val_acc:0.154000, test_acc:0.166000\n",
      "test accuracy:0.1740\n",
      "Epoch:4, loss=-395.2006, train_acc:0.1500, val_acc:0.148000, test_acc:0.181000\n",
      "test accuracy:0.1740\n",
      "Epoch:5, loss=-429.2599, train_acc:0.1333, val_acc:0.160000, test_acc:0.184000\n",
      "test accuracy:0.1740\n",
      "Epoch:6, loss=-433.9079, train_acc:0.1000, val_acc:0.148000, test_acc:0.167000\n",
      "test accuracy:0.1740\n",
      "Epoch:7, loss=-428.8533, train_acc:0.1333, val_acc:0.172000, test_acc:0.190000\n",
      "test accuracy:0.1740\n",
      "Epoch:8, loss=-426.6985, train_acc:0.1500, val_acc:0.166000, test_acc:0.186000\n",
      "test accuracy:0.1740\n",
      "Epoch:9, loss=-447.3349, train_acc:0.1833, val_acc:0.170000, test_acc:0.195000\n",
      "test accuracy:0.1740\n",
      "Epoch:10, loss=-452.0281, train_acc:0.1500, val_acc:0.158000, test_acc:0.179000\n",
      "test accuracy:0.1740\n",
      "Epoch:11, loss=-449.9693, train_acc:0.1500, val_acc:0.160000, test_acc:0.196000\n",
      "test accuracy:0.1740\n",
      "Epoch:12, loss=-453.5546, train_acc:0.2000, val_acc:0.150000, test_acc:0.179000\n",
      "test accuracy:0.1740\n",
      "Epoch:13, loss=-455.4533, train_acc:0.2000, val_acc:0.174000, test_acc:0.199000\n",
      "test accuracy:0.1740\n",
      "Epoch:14, loss=-452.1191, train_acc:0.2000, val_acc:0.174000, test_acc:0.192000\n",
      "test accuracy:0.1740\n",
      "Epoch:15, loss=-453.9892, train_acc:0.2167, val_acc:0.168000, test_acc:0.190000\n",
      "test accuracy:0.1740\n",
      "Epoch:16, loss=-458.5431, train_acc:0.1667, val_acc:0.150000, test_acc:0.180000\n",
      "test accuracy:0.1740\n",
      "Epoch:17, loss=-458.6844, train_acc:0.1667, val_acc:0.164000, test_acc:0.174000\n",
      "test accuracy:0.1740\n",
      "Epoch:18, loss=-463.5648, train_acc:0.2000, val_acc:0.150000, test_acc:0.182000\n",
      "test accuracy:0.1740\n",
      "Epoch:19, loss=-463.8434, train_acc:0.1833, val_acc:0.142000, test_acc:0.152000\n",
      "test accuracy:0.1740\n",
      "Epoch:20, loss=-464.1775, train_acc:0.2000, val_acc:0.154000, test_acc:0.172000\n",
      "test accuracy:0.1740\n",
      "Epoch:21, loss=-464.7481, train_acc:0.2000, val_acc:0.156000, test_acc:0.175000\n",
      "test accuracy:0.1740\n",
      "Epoch:22, loss=-465.4060, train_acc:0.2000, val_acc:0.168000, test_acc:0.172000\n",
      "test accuracy:0.1740\n",
      "Epoch:23, loss=-465.4976, train_acc:0.1833, val_acc:0.160000, test_acc:0.159000\n",
      "test accuracy:0.1740\n",
      "Epoch:24, loss=-467.8171, train_acc:0.1667, val_acc:0.146000, test_acc:0.158000\n",
      "test accuracy:0.1740\n",
      "Epoch:25, loss=-467.5417, train_acc:0.1833, val_acc:0.148000, test_acc:0.143000\n",
      "test accuracy:0.1740\n",
      "Epoch:26, loss=-468.6786, train_acc:0.1500, val_acc:0.138000, test_acc:0.157000\n",
      "test accuracy:0.1740\n",
      "Epoch:27, loss=-468.7695, train_acc:0.2000, val_acc:0.124000, test_acc:0.145000\n",
      "test accuracy:0.1740\n",
      "Epoch:28, loss=-467.4543, train_acc:0.1500, val_acc:0.122000, test_acc:0.130000\n",
      "test accuracy:0.1740\n",
      "Epoch:29, loss=-467.1134, train_acc:0.1667, val_acc:0.114000, test_acc:0.143000\n",
      "test accuracy:0.1740\n",
      "Epoch:30, loss=-469.3590, train_acc:0.1667, val_acc:0.090000, test_acc:0.128000\n",
      "test accuracy:0.1740\n",
      "Epoch:31, loss=-469.2374, train_acc:0.1667, val_acc:0.102000, test_acc:0.131000\n",
      "test accuracy:0.1740\n",
      "Epoch:32, loss=-465.7986, train_acc:0.1667, val_acc:0.086000, test_acc:0.117000\n",
      "test accuracy:0.1740\n",
      "Epoch:33, loss=-468.0560, train_acc:0.1667, val_acc:0.088000, test_acc:0.110000\n",
      "test accuracy:0.1740\n",
      "Epoch:34, loss=-466.5908, train_acc:0.1500, val_acc:0.076000, test_acc:0.130000\n",
      "test accuracy:0.1740\n",
      "Epoch:35, loss=-469.1279, train_acc:0.2000, val_acc:0.086000, test_acc:0.135000\n",
      "test accuracy:0.1740\n",
      "Epoch:36, loss=-468.5661, train_acc:0.1833, val_acc:0.092000, test_acc:0.131000\n",
      "test accuracy:0.1740\n",
      "Epoch:37, loss=-468.5024, train_acc:0.2167, val_acc:0.094000, test_acc:0.152000\n",
      "test accuracy:0.1740\n",
      "Epoch:38, loss=-467.3530, train_acc:0.1667, val_acc:0.112000, test_acc:0.146000\n",
      "test accuracy:0.1740\n",
      "Epoch:39, loss=-470.5926, train_acc:0.1833, val_acc:0.114000, test_acc:0.132000\n",
      "test accuracy:0.1740\n",
      "Epoch:40, loss=-468.0549, train_acc:0.1667, val_acc:0.104000, test_acc:0.142000\n",
      "test accuracy:0.1740\n",
      "Epoch:41, loss=-469.9931, train_acc:0.1833, val_acc:0.102000, test_acc:0.119000\n",
      "test accuracy:0.1740\n",
      "Epoch:42, loss=-468.1399, train_acc:0.1833, val_acc:0.096000, test_acc:0.118000\n",
      "test accuracy:0.1740\n",
      "Epoch:43, loss=-469.1812, train_acc:0.2500, val_acc:0.108000, test_acc:0.115000\n",
      "test accuracy:0.1740\n",
      "Epoch:44, loss=-470.7520, train_acc:0.2500, val_acc:0.122000, test_acc:0.116000\n",
      "test accuracy:0.1740\n",
      "Epoch:45, loss=-471.1108, train_acc:0.2500, val_acc:0.150000, test_acc:0.134000\n",
      "test accuracy:0.1740\n",
      "Epoch:46, loss=-470.9857, train_acc:0.2667, val_acc:0.184000, test_acc:0.157000\n",
      "test accuracy:0.1740\n",
      "Epoch:47, loss=-472.8644, train_acc:0.3167, val_acc:0.180000, test_acc:0.169000\n",
      "test accuracy:0.1740\n",
      "Epoch:48, loss=-470.6553, train_acc:0.3333, val_acc:0.196000, test_acc:0.184000\n",
      "test accuracy:0.1840\n",
      "Epoch:49, loss=-471.4453, train_acc:0.2833, val_acc:0.174000, test_acc:0.179000\n",
      "test accuracy:0.1840\n",
      "Epoch:50, loss=-472.8857, train_acc:0.3000, val_acc:0.190000, test_acc:0.173000\n",
      "test accuracy:0.1840\n",
      "Epoch:51, loss=-472.5518, train_acc:0.2833, val_acc:0.194000, test_acc:0.179000\n",
      "test accuracy:0.1840\n",
      "Epoch:52, loss=-471.4895, train_acc:0.2833, val_acc:0.188000, test_acc:0.178000\n",
      "test accuracy:0.1840\n",
      "Epoch:53, loss=-469.6332, train_acc:0.3500, val_acc:0.194000, test_acc:0.174000\n",
      "test accuracy:0.1840\n",
      "Epoch:54, loss=-471.7608, train_acc:0.3167, val_acc:0.188000, test_acc:0.157000\n",
      "test accuracy:0.1840\n",
      "Epoch:55, loss=-473.3933, train_acc:0.3000, val_acc:0.170000, test_acc:0.151000\n",
      "test accuracy:0.1840\n",
      "Epoch:56, loss=-471.2802, train_acc:0.3000, val_acc:0.154000, test_acc:0.138000\n",
      "test accuracy:0.1840\n",
      "Epoch:57, loss=-472.1029, train_acc:0.2500, val_acc:0.104000, test_acc:0.098000\n",
      "test accuracy:0.1840\n",
      "Epoch:58, loss=-470.7732, train_acc:0.0833, val_acc:0.034000, test_acc:0.038000\n",
      "test accuracy:0.1840\n",
      "Epoch:59, loss=-473.4696, train_acc:0.0333, val_acc:0.010000, test_acc:0.019000\n",
      "test accuracy:0.1840\n",
      "Epoch:60, loss=-472.6553, train_acc:0.0167, val_acc:0.008000, test_acc:0.018000\n",
      "test accuracy:0.1840\n",
      "Epoch:61, loss=-474.0114, train_acc:0.0000, val_acc:0.020000, test_acc:0.016000\n",
      "test accuracy:0.1840\n",
      "Epoch:62, loss=-473.1734, train_acc:0.0167, val_acc:0.020000, test_acc:0.025000\n",
      "test accuracy:0.1840\n",
      "Epoch:63, loss=-471.9894, train_acc:0.0167, val_acc:0.028000, test_acc:0.023000\n",
      "test accuracy:0.1840\n",
      "Epoch:64, loss=-473.2476, train_acc:0.0167, val_acc:0.022000, test_acc:0.028000\n",
      "test accuracy:0.1840\n",
      "Epoch:65, loss=-475.3942, train_acc:0.0000, val_acc:0.024000, test_acc:0.025000\n",
      "test accuracy:0.1840\n",
      "Epoch:66, loss=-475.0069, train_acc:0.0500, val_acc:0.022000, test_acc:0.024000\n",
      "test accuracy:0.1840\n",
      "Epoch:67, loss=-474.6822, train_acc:0.0167, val_acc:0.026000, test_acc:0.021000\n",
      "test accuracy:0.1840\n",
      "Epoch:68, loss=-475.4943, train_acc:0.0167, val_acc:0.026000, test_acc:0.020000\n",
      "test accuracy:0.1840\n",
      "Epoch:69, loss=-473.8573, train_acc:0.0333, val_acc:0.038000, test_acc:0.032000\n",
      "test accuracy:0.1840\n",
      "Epoch:70, loss=-473.6725, train_acc:0.0500, val_acc:0.038000, test_acc:0.031000\n",
      "test accuracy:0.1840\n",
      "Epoch:71, loss=-476.3785, train_acc:0.0167, val_acc:0.034000, test_acc:0.046000\n",
      "test accuracy:0.1840\n",
      "Epoch:72, loss=-474.5637, train_acc:0.0667, val_acc:0.044000, test_acc:0.051000\n",
      "test accuracy:0.1840\n",
      "Epoch:73, loss=-473.5054, train_acc:0.0833, val_acc:0.056000, test_acc:0.061000\n",
      "test accuracy:0.1840\n",
      "Epoch:74, loss=-476.5905, train_acc:0.0833, val_acc:0.052000, test_acc:0.062000\n",
      "test accuracy:0.1840\n",
      "Epoch:75, loss=-476.0624, train_acc:0.1167, val_acc:0.050000, test_acc:0.056000\n",
      "test accuracy:0.1840\n",
      "Epoch:76, loss=-475.4229, train_acc:0.1167, val_acc:0.070000, test_acc:0.063000\n",
      "test accuracy:0.1840\n",
      "Epoch:77, loss=-478.0518, train_acc:0.1333, val_acc:0.054000, test_acc:0.055000\n",
      "test accuracy:0.1840\n",
      "Epoch:78, loss=-476.4642, train_acc:0.1000, val_acc:0.060000, test_acc:0.062000\n",
      "test accuracy:0.1840\n",
      "Epoch:79, loss=-476.4801, train_acc:0.1500, val_acc:0.054000, test_acc:0.068000\n",
      "test accuracy:0.1840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:80, loss=-477.0278, train_acc:0.1333, val_acc:0.054000, test_acc:0.067000\n",
      "test accuracy:0.1840\n",
      "Epoch:81, loss=-476.3353, train_acc:0.1000, val_acc:0.056000, test_acc:0.065000\n",
      "test accuracy:0.1840\n",
      "Epoch:82, loss=-476.2201, train_acc:0.1167, val_acc:0.056000, test_acc:0.074000\n",
      "test accuracy:0.1840\n",
      "Epoch:83, loss=-475.2142, train_acc:0.1333, val_acc:0.062000, test_acc:0.080000\n",
      "test accuracy:0.1840\n",
      "Epoch:84, loss=-476.8502, train_acc:0.1500, val_acc:0.048000, test_acc:0.078000\n",
      "test accuracy:0.1840\n",
      "Epoch:85, loss=-476.4197, train_acc:0.1500, val_acc:0.064000, test_acc:0.073000\n",
      "test accuracy:0.1840\n",
      "Epoch:86, loss=-478.0865, train_acc:0.1000, val_acc:0.054000, test_acc:0.073000\n",
      "test accuracy:0.1840\n",
      "Epoch:87, loss=-476.3641, train_acc:0.1500, val_acc:0.072000, test_acc:0.078000\n",
      "test accuracy:0.1840\n",
      "Epoch:88, loss=-476.8681, train_acc:0.1333, val_acc:0.074000, test_acc:0.070000\n",
      "test accuracy:0.1840\n",
      "Epoch:89, loss=-476.2078, train_acc:0.1000, val_acc:0.054000, test_acc:0.067000\n",
      "test accuracy:0.1840\n",
      "Epoch:90, loss=-477.7661, train_acc:0.1000, val_acc:0.060000, test_acc:0.059000\n",
      "test accuracy:0.1840\n",
      "Epoch:91, loss=-479.5840, train_acc:0.0833, val_acc:0.072000, test_acc:0.071000\n",
      "test accuracy:0.1840\n",
      "Epoch:92, loss=-478.0778, train_acc:0.1000, val_acc:0.090000, test_acc:0.074000\n",
      "test accuracy:0.1840\n",
      "Epoch:93, loss=-473.1367, train_acc:0.0833, val_acc:0.082000, test_acc:0.082000\n",
      "test accuracy:0.1840\n",
      "Epoch:94, loss=-478.8279, train_acc:0.1167, val_acc:0.094000, test_acc:0.071000\n",
      "test accuracy:0.1840\n",
      "Epoch:95, loss=-476.9839, train_acc:0.1000, val_acc:0.090000, test_acc:0.074000\n",
      "test accuracy:0.1840\n",
      "Epoch:96, loss=-476.1245, train_acc:0.0833, val_acc:0.114000, test_acc:0.088000\n",
      "test accuracy:0.1840\n",
      "Epoch:97, loss=-480.0986, train_acc:0.0500, val_acc:0.110000, test_acc:0.090000\n",
      "test accuracy:0.1840\n",
      "Epoch:98, loss=-471.8078, train_acc:0.0500, val_acc:0.108000, test_acc:0.087000\n",
      "test accuracy:0.1840\n",
      "Epoch:99, loss=-476.9655, train_acc:0.0500, val_acc:0.116000, test_acc:0.079000\n",
      "test accuracy:0.1840\n"
     ]
    }
   ],
   "source": [
    "graph, feat, labels, num_class, train_idx, val_idx, test_idx = load('pubmed')\n",
    "graph = graph.remove_self_loop().add_self_loop()\n",
    "\n",
    "in_dim = feat.shape[1]\n",
    "\n",
    "hid_dim = 512\n",
    "out_dim = 512\n",
    "n_layers = 2\n",
    "n_classes = 7\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "label = labels\n",
    "train_labels = label[train_idx]\n",
    "val_labels = label[val_idx]\n",
    "test_labels = label[test_idx]\n",
    "    \n",
    "best_val_acc = 0\n",
    "eval_acc = 0\n",
    "\n",
    "#best_val_acc2 = 0 \n",
    "#eval_acc2 = 0 \n",
    "\n",
    "model = TOCCA(in_dim, hid_dim, out_dim, n_classes, n_layers, use_mlp=False)\n",
    "lr1 = 1e-3\n",
    "wd1 = 0\n",
    "wd2 = 1e-4\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=lr1, weight_decay=wd2)\n",
    "\n",
    "N = graph.number_of_nodes()\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    dfr = 0.2\n",
    "    der = 0.2\n",
    "\n",
    "    graph1, feat1 = random_aug(graph, feat, 0.2, 0.2)\n",
    "    graph2, feat2 = random_aug(graph, feat, 0.2, 0.2)\n",
    "\n",
    "    graph1 = graph1.add_self_loop()\n",
    "    graph2 = graph2.add_self_loop()\n",
    "\n",
    "    z1, z2, logits = model(graph1, feat1, graph2, feat2)\n",
    "    # z1, z2, logits, logits2 = model(graph1, feat1, graph2, feat2)\n",
    "\n",
    "    c = th.mm(z1.T, z2)\n",
    "    c1 = th.mm(z1.T, z1)\n",
    "    c2 = th.mm(z2.T, z2)\n",
    "\n",
    "    c = c / N\n",
    "    c1 = c1 / N\n",
    "    c2 = c2 / N\n",
    "\n",
    "    loss_inv = -th.diagonal(c).sum()\n",
    "    iden = th.tensor(np.eye(c.shape[0]))\n",
    "    loss_dec1 = (iden - c1).pow(2).sum()\n",
    "    loss_dec2 = (iden - c2).pow(2).sum()\n",
    "\n",
    "    lambd1 = 0 # best 1e-2\n",
    "    lambd2 = 1e-4\n",
    "\n",
    "    train_embs = logits[train_idx]\n",
    "    val_embs = logits[val_idx]\n",
    "    test_embs = logits[test_idx]\n",
    "\n",
    "    # train_embs2 = logits2[train_idx] #\n",
    "    # val_embs2 = logits2[val_idx] #\n",
    "    # test_embs2 = logits2[test_idx] #\n",
    "\n",
    "    preds = th.argmax(train_embs, dim=1)\n",
    "    # preds2 = th.argmax(train_embs2, dim=1) #\n",
    "\n",
    "    train_acc = th.sum(preds == train_labels).float() / train_labels.shape[0]\n",
    "    # train_acc2 = th.sum(preds2 == train_labels).float() / train_labels.shape[0] #\n",
    "    loss_task = loss_fn(train_embs, train_labels)\n",
    "    # loss_task2 = loss_fn(train_embs2, train_labels) #\n",
    "\n",
    "    loss = lambd1 * loss_task + loss_inv + lambd2 * (loss_dec1 + loss_dec2) # \n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        val_preds = th.argmax(val_embs, dim=1)\n",
    "        test_preds = th.argmax(test_embs, dim=1)\n",
    "        # val_preds2 = th.argmax(val_embs2, dim=1)\n",
    "        # test_preds2 = th.argmax(test_embs2, dim=1)\n",
    "\n",
    "        val_acc = th.sum(val_preds == val_labels).float() / val_labels.shape[0]\n",
    "        test_acc = th.sum(test_preds == test_labels).float() / test_labels.shape[0]\n",
    "        #val_acc2 = th.sum(val_preds2 == val_labels).float() / val_labels.shape[0]\n",
    "        #test_acc2 = th.sum(test_preds2 == test_labels).float() / test_labels.shape[0]\n",
    "\n",
    "        if val_acc >= best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            if test_acc > eval_acc:\n",
    "                eval_acc = test_acc\n",
    "\n",
    "    print('Epoch:{}, loss={:.4f}, train_acc:{:.4f}, val_acc:{:4f}, test_acc:{:4f}'.format(epoch, loss.item(), train_acc, val_acc, test_acc))\n",
    "    print('test accuracy:{:.4f}'.format(eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1ehew5gq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">polar-darkness-3</strong>: <a href=\"https://wandb.ai/graphs-stats-and-ml/VAE/runs/1ehew5gq\" target=\"_blank\">https://wandb.ai/graphs-stats-and-ml/VAE/runs/1ehew5gq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220422_103556-1ehew5gq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1ehew5gq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ilgeehong/Desktop/UChicago/Research/Graph/VAE Project/Code/Project Code/TOCCA/wandb/run-20220422_103607-361bxs28</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/graphs-stats-and-ml/Project%20Code-TOCCA/runs/361bxs28\" target=\"_blank\">lunar-snowflake-3</a></strong> to <a href=\"https://wandb.ai/graphs-stats-and-ml/Project%20Code-TOCCA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb.init(config={\"lambd1\": 0})\n",
    "# wandb.log({'accuracy': eval_acc})\n",
    "# wandb.config.update({\"lr\": 0.1, \"channels\": 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citeseer = [71.2, 70.8, 70.9, 71.3, 70.3]\n",
    "cora = [82, 80.5, 81.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "graph, feat, labels, num_class, train_idx, val_idx, test_idx = load('cora')\n",
    "graph = graph.remove_self_loop().add_self_loop()\n",
    "\n",
    "in_dim = feat.shape[1]\n",
    "\n",
    "hid_dim = 512\n",
    "out_dim = 512\n",
    "n_layers = 2\n",
    "n_classes = 7\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "label = labels\n",
    "train_labels = label[train_idx]\n",
    "val_labels = label[val_idx]\n",
    "test_labels = label[test_idx]\n",
    "    \n",
    "best_val_acc = 0\n",
    "eval_acc = 0\n",
    "\n",
    "#best_val_acc2 = 0 \n",
    "#eval_acc2 = 0 \n",
    "\n",
    "model = TOCCA(in_dim, hid_dim, out_dim, n_classes, n_layers, use_mlp=False)\n",
    "lr1 = 1e-3\n",
    "wd1 = 0\n",
    "wd2 = 1e-4\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=lr1, weight_decay=wd2)\n",
    "\n",
    "N = graph.number_of_nodes()\n",
    "\n",
    "lambda2_list = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6] \n",
    "lambda2_data = th.zeros((len(lambda2_list), 5))\n",
    "\n",
    "for j, lambda2_value in enumerate(lambda2_list):\n",
    "    for n in range(5):\n",
    "        for epoch in range(100):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            dfr = 0.2\n",
    "            der = 0.2\n",
    "\n",
    "            graph1, feat1 = random_aug(graph, feat, 0.2, 0.2)\n",
    "            graph2, feat2 = random_aug(graph, feat, 0.2, 0.2)\n",
    "\n",
    "            graph1 = graph1.add_self_loop()\n",
    "            graph2 = graph2.add_self_loop()\n",
    "\n",
    "            z1, z2, logits = model(graph1, feat1, graph2, feat2)\n",
    "            # z1, z2, logits, logits2 = model(graph1, feat1, graph2, feat2)\n",
    "\n",
    "            c = th.mm(z1.T, z2)\n",
    "            c1 = th.mm(z1.T, z1)\n",
    "            c2 = th.mm(z2.T, z2)\n",
    "\n",
    "            c = c / N\n",
    "            c1 = c1 / N\n",
    "            c2 = c2 / N\n",
    "\n",
    "            loss_inv = -th.diagonal(c).sum()\n",
    "            iden = th.tensor(np.eye(c.shape[0]))\n",
    "            loss_dec1 = (iden - c1).pow(2).sum()\n",
    "            loss_dec2 = (iden - c2).pow(2).sum()\n",
    "\n",
    "            lambd1 = 10^2\n",
    "            lambd2 = lambda2_value # 1e-5\n",
    "\n",
    "            train_embs = logits[train_idx]\n",
    "            val_embs = logits[val_idx]\n",
    "            test_embs = logits[test_idx]\n",
    "\n",
    "            # train_embs2 = logits2[train_idx] #\n",
    "            # val_embs2 = logits2[val_idx] #\n",
    "            # test_embs2 = logits2[test_idx] #\n",
    "\n",
    "            preds = th.argmax(train_embs, dim=1)\n",
    "            # preds2 = th.argmax(train_embs2, dim=1) #\n",
    "\n",
    "            train_acc = th.sum(preds == train_labels).float() / train_labels.shape[0]\n",
    "            # train_acc2 = th.sum(preds2 == train_labels).float() / train_labels.shape[0] #\n",
    "            loss_task = loss_fn(train_embs, train_labels)\n",
    "            # loss_task2 = loss_fn(train_embs2, train_labels) #\n",
    "\n",
    "            loss = loss_task + lambd1 * loss_inv + lambd2 * (loss_dec1 + loss_dec2) # \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with th.no_grad():\n",
    "                val_preds = th.argmax(val_embs, dim=1)\n",
    "                test_preds = th.argmax(test_embs, dim=1)\n",
    "                # val_preds2 = th.argmax(val_embs2, dim=1)\n",
    "                # test_preds2 = th.argmax(test_embs2, dim=1)\n",
    "\n",
    "                val_acc = th.sum(val_preds == val_labels).float() / val_labels.shape[0]\n",
    "                test_acc = th.sum(test_preds == test_labels).float() / test_labels.shape[0]\n",
    "                #val_acc2 = th.sum(val_preds2 == val_labels).float() / val_labels.shape[0]\n",
    "                #test_acc2 = th.sum(test_preds2 == test_labels).float() / test_labels.shape[0]\n",
    "\n",
    "                if val_acc >= best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    if test_acc > eval_acc:\n",
    "                        eval_acc = test_acc\n",
    "\n",
    "            #print('Epoch:{}, loss={:.4f}, train_acc:{:.4f}, val_acc:{:4f}, test_acc:{:4f}'.format(epoch, loss.item(), train_acc, val_acc, test_acc))\n",
    "            #print('test accuracy:{:.4f}'.format(eval_acc))\n",
    "        lambda2_data[j, n] = eval_acc\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
